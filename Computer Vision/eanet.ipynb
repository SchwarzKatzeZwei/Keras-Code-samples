{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUSqH4V5xaTx"
      },
      "source": [
        "# Image classification with EANet (External Attention Transformer) (EANet（外部注意変換器）による画像分類)\n",
        "\n",
        "**Author:** [ZhiYong Chang](https://github.com/czy00000)<br>\n",
        "**Date created:** 2021/10/19<br>\n",
        "**Last modified:** 2021/10/19<br>\n",
        "**Description:** Image classification with a Transformer that leverages external attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OfpiHFAxaT0"
      },
      "source": [
        "## 序章\n",
        "\n",
        "この例では、[EANet](https://arxiv.org/abs/2105.02358)でモデルを実装し、CIFAR-100データセットで実証しています。\n",
        "EANetでは、新しい注目メカニズムを導入しています。\n",
        "これは、2つの外部記憶、小さくて学習可能な記憶、そして共有された記憶に基づいています。\n",
        "これは、カスケード接続された2つの線形層と2つの正規化層を使用するだけで簡単に実装できます。\n",
        "既存のアーキテクチャで使用されている自己注意を便利に置き換えるものです。\n",
        "外部注意は線形的な複雑さを持っており、それはすべてのサンプル間の相関関係を暗黙的に考慮するだけなので、線形の複雑さを持ちます。\n",
        "この例では、TensorFlow 2.5以上と、以下のものが必要です。\n",
        "[TensorFlow Addons](https://www.tensorflow.org/addons/overview)パッケージが必要です。\n",
        "これは、以下のコマンドでインストールできます。\n",
        "\n",
        "```python\n",
        "pip install -U tensorflow-addons\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOg7301JxaT1"
      },
      "source": [
        "## 設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KSUB5MpJxaT1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shogo/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx3DHWjPxaT2"
      },
      "source": [
        "## データの準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pru82xz2xaT3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 49s 0us/step\n",
            "169017344/169001437 [==============================] - 49s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 100)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 100)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVdJQLxrxaT3"
      },
      "source": [
        "## ハイパーパラメータの設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XfL5p7qNxaT4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patch size: 2 X 2 = 4 \n",
            "Patches per image: 256\n"
          ]
        }
      ],
      "source": [
        "weight_decay = 0.0001\n",
        "learning_rate = 0.001\n",
        "label_smoothing = 0.1\n",
        "validation_split = 0.2\n",
        "batch_size = 128\n",
        "num_epochs = 50\n",
        "patch_size = 2  # Size of the patches to be extracted from the input images.\n",
        "num_patches = (input_shape[0] // patch_size) ** 2  # Number of patch\n",
        "embedding_dim = 64  # Number of hidden units.\n",
        "mlp_dim = 64\n",
        "dim_coefficient = 4\n",
        "num_heads = 4\n",
        "attention_dropout = 0.2\n",
        "projection_dropout = 0.2\n",
        "num_transformer_blocks = 8  # Number of repetitions of the transformer layer\n",
        "\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDuxQin1xaT5"
      },
      "source": [
        "## データ増強の活用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-LQUet9fxaT5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M1\n",
            "\n",
            "systemMemory: 16.00 GB\n",
            "maxCacheSize: 5.33 GB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.1),\n",
        "        layers.RandomContrast(factor=0.1),\n",
        "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2xDZMhsxaT6"
      },
      "source": [
        "## パッチ抽出・エンコード層の実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Xi-NP1rZxaT6"
      },
      "outputs": [],
      "source": [
        "class PatchExtract(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super(PatchExtract, self).__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=(1, self.patch_size, self.patch_size, 1),\n",
        "            strides=(1, self.patch_size, self.patch_size, 1),\n",
        "            rates=(1, 1, 1, 1),\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dim = patches.shape[-1]\n",
        "        patch_num = patches.shape[1]\n",
        "        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n",
        "\n",
        "\n",
        "class PatchEmbedding(layers.Layer):\n",
        "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
        "        super(PatchEmbedding, self).__init__(**kwargs)\n",
        "        self.num_patch = num_patch\n",
        "        self.proj = layers.Dense(embed_dim)\n",
        "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, patch):\n",
        "        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n",
        "        return self.proj(patch) + self.pos_embed(pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK4meMchxaT7"
      },
      "source": [
        "## 外部注目ブロックの実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cNhbpqkuxaT7"
      },
      "outputs": [],
      "source": [
        "def external_attention(\n",
        "    x, dim, num_heads, dim_coefficient=4, attention_dropout=0, projection_dropout=0\n",
        "):\n",
        "    _, num_patch, channel = x.shape\n",
        "    assert dim % num_heads == 0\n",
        "    num_heads = num_heads * dim_coefficient\n",
        "\n",
        "    x = layers.Dense(dim * dim_coefficient)(x)\n",
        "    # create tensor [batch_size, num_patches, num_heads, dim*dim_coefficient//num_heads]\n",
        "    x = tf.reshape(\n",
        "        x, shape=(-1, num_patch, num_heads, dim * dim_coefficient // num_heads)\n",
        "    )\n",
        "    x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    # a linear layer M_k\n",
        "    attn = layers.Dense(dim // dim_coefficient)(x)\n",
        "    # normalize attention map\n",
        "    attn = layers.Softmax(axis=2)(attn)\n",
        "    # dobule-normalization\n",
        "    attn = attn / (1e-9 + tf.reduce_sum(attn, axis=-1, keepdims=True))\n",
        "    attn = layers.Dropout(attention_dropout)(attn)\n",
        "    # a linear layer M_v\n",
        "    x = layers.Dense(dim * dim_coefficient // num_heads)(attn)\n",
        "    x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    x = tf.reshape(x, [-1, num_patch, dim * dim_coefficient])\n",
        "    # a linear layer to project original dim\n",
        "    x = layers.Dense(dim)(x)\n",
        "    x = layers.Dropout(projection_dropout)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wj4yhjBxaT7"
      },
      "source": [
        "## MLPブロックの実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "g4eAr95sxaT8"
      },
      "outputs": [],
      "source": [
        "def mlp(x, embedding_dim, mlp_dim, drop_rate=0.2):\n",
        "    x = layers.Dense(mlp_dim, activation=tf.nn.gelu)(x)\n",
        "    x = layers.Dropout(drop_rate)(x)\n",
        "    x = layers.Dense(embedding_dim)(x)\n",
        "    x = layers.Dropout(drop_rate)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWL17_EPxaT8"
      },
      "source": [
        "## トランスフォーマーブロックの実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eeQdXbL3xaT8"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(\n",
        "    x,\n",
        "    embedding_dim,\n",
        "    mlp_dim,\n",
        "    num_heads,\n",
        "    dim_coefficient,\n",
        "    attention_dropout,\n",
        "    projection_dropout,\n",
        "    attention_type=\"external_attention\",\n",
        "):\n",
        "    residual_1 = x\n",
        "    x = layers.LayerNormalization(epsilon=1e-5)(x)\n",
        "    if attention_type == \"external_attention\":\n",
        "        x = external_attention(\n",
        "            x,\n",
        "            embedding_dim,\n",
        "            num_heads,\n",
        "            dim_coefficient,\n",
        "            attention_dropout,\n",
        "            projection_dropout,\n",
        "        )\n",
        "    elif attention_type == \"self_attention\":\n",
        "        x = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embedding_dim, dropout=attention_dropout\n",
        "        )(x, x)\n",
        "    x = layers.add([x, residual_1])\n",
        "    residual_2 = x\n",
        "    x = layers.LayerNormalization(epsilon=1e-5)(x)\n",
        "    x = mlp(x, embedding_dim, mlp_dim)\n",
        "    x = layers.add([x, residual_2])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXS-B2tAxaT8"
      },
      "source": [
        "## EANetモデルの導入"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJgQPq0BxaT8"
      },
      "source": [
        "EANetモデルでは、外部からの注意を活用します。\n",
        "従来のセルフアテンションの計算量は，`O(d * N ** 2)`であり、ここで`d` は埋め込みサイズ、`N` はパッチの数です。\n",
        "著者らは，ほとんどのピクセルがわずか数個の他のピクセルと密接に関連していることを発見し、`N`対`N`のアテンションマトリックスは冗長になる可能性があることを発見しました。\n",
        "そこで彼らは代替案として、外部アテンションの計算量は`O(d * S * N)`としました。\n",
        "`d`と`S`はハイパーパラメータです。\n",
        "提案されたアルゴリズムはピクセル数に対して線形です。\n",
        "実際には画像のパッチに含まれる多くの情報は冗長であり、不要であるため、これはドロップパッチ操作と同等です。\n",
        "画像のパッチに含まれる多くの情報は冗長で重要ではないからです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6HEcLmlVxaT9"
      },
      "outputs": [],
      "source": [
        "def get_model(attention_type=\"external_attention\"):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Image augment\n",
        "    x = data_augmentation(inputs)\n",
        "    # Extract patches.\n",
        "    x = PatchExtract(patch_size)(x)\n",
        "    # Create patch embedding.\n",
        "    x = PatchEmbedding(num_patches, embedding_dim)(x)\n",
        "    # Create Transformer block.\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(\n",
        "            x,\n",
        "            embedding_dim,\n",
        "            mlp_dim,\n",
        "            num_heads,\n",
        "            dim_coefficient,\n",
        "            attention_dropout,\n",
        "            projection_dropout,\n",
        "            attention_type,\n",
        "        )\n",
        "\n",
        "    x = layers.GlobalAvgPool1D()(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P3LSMOwxaT9"
      },
      "source": [
        "## CIFAR-100でのトレーニング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DHof4dE1xaT9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Cannot assign a device for operation model_1/data_augmentation/random_flip/stateful_uniform_full_int/RngReadAndSkip: Could not satisfy explicit device specification '' because the node {{colocation_node model_1/data_augmentation/random_flip/stateful_uniform_full_int/RngReadAndSkip}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. \nColocation Debug Info:\nColocation group had the following types and supported devices: \nRoot Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\nRngReadAndSkip: CPU \n_Arg: GPU CPU \n\nColocation members, user-requested devices, and framework assigned devices, if any:\n  model_1_data_augmentation_random_flip_stateful_uniform_full_int_rngreadandskip_resource (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  model_1/data_augmentation/random_flip/stateful_uniform_full_int/RngReadAndSkip (RngReadAndSkip) \n\n\t [[{{node model_1/data_augmentation/random_flip/stateful_uniform_full_int/RngReadAndSkip}}]] [Op:__inference_train_function_36871]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/83/7ztpck1529700b59mz_yqpw00000gn/T/ipykernel_73835/3506624781.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation model_1/data_augmentation/random_flip/stateful_uniform_full_int/RngReadAndSkip: Could not satisfy explicit device specification '' because the node {{colocation_node model_1/data_augmentation/random_flip/stateful_uniform_full_int/RngReadAndSkip}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. \nColocation Debug Info:\nColocation group had the following types and supported devices: \nRoot Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\nRngReadAndSkip: CPU \n_Arg: GPU CPU \n\nColocation members, user-requested devices, and framework assigned devices, if any:\n  model_1_data_augmentation_random_flip_stateful_uniform_full_int_rngreadandskip_resource (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  model_1/data_augmentation/random_flip/stateful_uniform_full_int/RngReadAndSkip (RngReadAndSkip) \n\n\t [[{{node model_1/data_augmentation/random_flip/stateful_uniform_full_int/RngReadAndSkip}}]] [Op:__inference_train_function_36871]"
          ]
        }
      ],
      "source": [
        "model = get_model(attention_type=\"external_attention\")\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
        "    optimizer=tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    ),\n",
        "    metrics=[\n",
        "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs,\n",
        "    validation_split=validation_split,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8LmQ-aExaT9"
      },
      "source": [
        "### モデルの学習進捗状況を可視化してみましょう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4oieSowxaT9"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgRtuxwvxaT9"
      },
      "source": [
        "### CIFAR-100でのテストの最終結果を表示してみましょう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cV5f9V1xaT9"
      },
      "outputs": [],
      "source": [
        "loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test loss: {round(loss, 2)}\")\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb_J-GaSxaT-"
      },
      "source": [
        "EANetは、Vitの自己注意を外部注意に置き換えるだけです。\n",
        "従来のVitでは、50エポックの学習後、テスト5位以内の精度が73%程度、1位以内の精度が41%程度でした。\n",
        "同じ実験環境、同じハイパーパラメータの下では、今回学習したEANetモデルのパラメータはわずか0.3Mです。\n",
        "このモデルでは、テストのトップ5の精度が73％、トップ1の精度が43％となりました。\n",
        "これは、外部からの注目の有効性を完全に証明しています。\n",
        "ここでは、EANetの学習プロセスのみを紹介しています。\n",
        "同じ実験条件でVitをトレーニングし、テスト結果を観察することができます。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "EANet",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
