{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4-R_0Cf-zpw"
      },
      "source": [
        "# Compact Convolutional Transformers (コンパクトな畳み込みトランス)\n",
        "\n",
        "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
        "**Date created:** 2021/06/30<br>\n",
        "**Last modified:** 2021/06/30<br>\n",
        "**Description:** Compact Convolutional Transformers for efficient image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w7_jXji-zpz"
      },
      "source": [
        "[Vision Transformers (ViT)](https://arxiv.org/abs/2010.11929)の論文で述べたように、\n",
        "視覚のためのTransformerベースのアーキテクチャは、通常よりも大きなデータセットを必要とします。\n",
        "また、事前学習のスケジュールも長くなります。[ImageNet-1k](http://imagenet.org/)\n",
        "(約100万枚の画像)は、ViTsに関しては中規模のデータに該当すると考えられます。\n",
        "これは主に、CNNとは異なり、ViT（または典型的なTransformerベースのアーキテクチャ）は、\n",
        "十分な情報を持った帰納的バイアス（画像処理のための畳み込みなど）を持っていないからです。\n",
        "畳み込みの利点とトランスフォーマーの利点を組み合わせることができないか、という疑問が湧いてきます。\n",
        "その利点とは、パラメータの効率性、そして長距離・大域的な依存性（画像内の異なる領域間の相互作用）を処理するための自己調整機能です。\n",
        "\n",
        "[Escaping the Big Data Paradigm with Compact Transformers (コンパクトなトランスでビッグデータパラダイムからの脱出を図る)](https://arxiv.org/abs/2104.05704)では、\n",
        "Hassaniらは、まさにこれを行うためのアプローチを提示しています。彼らが提案したのは\n",
        "**Compact Convolutional Transformer** (CCT)アーキテクチャです。\n",
        "この例では、CCTの実装を行い、CIFAR-10データセットでどの程度の性能を発揮するかを見ていきます。\n",
        "\n",
        "自己言及やトランスフォーマーの概念に馴染みのない方には\n",
        "[この章](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11/r-3/312)\n",
        "François Chollet氏の著書、*Deep Learning with Python*をお読みください。\n",
        "この例では、別の例からのコードスニペットを使用しています。\n",
        "[Image classification with Vision Transformer (Vision Transformerによる画像分類)](https://keras.io/examples/vision/image_classification_with_vision_transformer/)を参照してください。\n",
        "\n",
        "この例では、TensorFlow 2.5以降とTensorFlow Addonsが必要で、これらは次のコマンドでインストールできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f59UHhV9-zpz"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q tensorflow-addons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## インポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8q5kd0Ua-zp0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zcb1tnD-zp1"
      },
      "source": [
        "## ハイパーパラメータとコンスタント"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o-IA2q_p-zp1"
      },
      "outputs": [],
      "source": [
        "positional_emb = True\n",
        "conv_layers = 2\n",
        "projection_dim = 128\n",
        "\n",
        "num_heads = 2\n",
        "transformer_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 2\n",
        "stochastic_depth_rate = 0.1\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "image_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV-hoyWd-zp2"
      },
      "source": [
        "## CIFAR-10データセットの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Hy28clx9-zp2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 363s 2us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 10)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7mcVyn-zp3"
      },
      "source": [
        "## CCT トークナイザー\n",
        "\n",
        "CCTの著者が導入した最初のレシピは、画像を処理するためのトークン化です。\n",
        "標準的なViTでは、画像はオーバーラップしていない均一なパッチに整理されます。\n",
        "これにより、異なるパッチの間に存在する境界レベルの情報が排除されます。\n",
        "これは、ニューラルネットワークが位置情報を効果的に利用するために重要です。\n",
        "下の図は、画像がどのようにパッチに整理されるかを示しています。\n",
        "\n",
        "![](https://i.imgur.com/IkBK9oY.png)\n",
        "\n",
        "畳み込みは位置情報を利用するのに非常に適していることがすでにわかっています。そこで\n",
        "これに基づいて、著者らは、画像パッチを生成するために、すべてのコンボリューション・ミニ・ネットワークを導入しました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mgfhl_xx-zp3"
      },
      "outputs": [],
      "source": [
        "class CCTTokenizer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        pooling_kernel_size=3,\n",
        "        pooling_stride=2,\n",
        "        num_conv_layers=conv_layers,\n",
        "        num_output_channels=[64, 128],\n",
        "        positional_emb=positional_emb,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(CCTTokenizer, self).__init__(**kwargs)\n",
        "\n",
        "        # This is our tokenizer.\n",
        "        self.conv_model = keras.Sequential()\n",
        "        for i in range(num_conv_layers):\n",
        "            self.conv_model.add(\n",
        "                layers.Conv2D(\n",
        "                    num_output_channels[i],\n",
        "                    kernel_size,\n",
        "                    stride,\n",
        "                    padding=\"valid\",\n",
        "                    use_bias=False,\n",
        "                    activation=\"relu\",\n",
        "                    kernel_initializer=\"he_normal\",\n",
        "                )\n",
        "            )\n",
        "            self.conv_model.add(layers.ZeroPadding2D(padding))\n",
        "            self.conv_model.add(\n",
        "                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n",
        "            )\n",
        "\n",
        "        self.positional_emb = positional_emb\n",
        "\n",
        "    def call(self, images):\n",
        "        outputs = self.conv_model(images)\n",
        "        # 画像をミニネットワークに通した後、\n",
        "        # 空間的な次元を シーケンスに変換\n",
        "        reshaped = tf.reshape(\n",
        "            outputs,\n",
        "            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
        "        )\n",
        "        return reshaped\n",
        "\n",
        "    def positional_embedding(self, image_size):\n",
        "        # 位置エンベッディングはCCTではオプションです\n",
        "        # ここでは、シーケンスの数を計算し、\n",
        "        # `Embedding`レイヤーを初期化します\n",
        "        if self.positional_emb:\n",
        "            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
        "            dummy_outputs = self.call(dummy_inputs)\n",
        "            sequence_length = tf.shape(dummy_outputs)[1]\n",
        "            projection_dim = tf.shape(dummy_outputs)[-1]\n",
        "\n",
        "            embed_layer = layers.Embedding(\n",
        "                input_dim=sequence_length, output_dim=projection_dim\n",
        "            )\n",
        "            return embed_layer, sequence_length\n",
        "        else:\n",
        "            return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY1t6EP2-zp4"
      },
      "source": [
        "## 正則化のためのストキャスティック・デプス\n",
        "\n",
        "[Stochastic depth](https://arxiv.org/abs/1603.09382)は、レイヤーのセットをランダムに落とす正則化の手法です。\n",
        "レイヤのセットをランダムに削除します。推論の際には、層はそのまま維持されます。\n",
        "これは[Dropout](https://jmlr.org/papers/v15/srivastava14a.html)とよく似ていますが、\n",
        "層の中に存在する個々のノードではなく、層のブロックを操作することだけです。\n",
        "CCTでは、Transformersエンコーダの残差ブロックの直前にstochastic depthを使用しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3EhEsTU7-zp4"
      },
      "outputs": [],
      "source": [
        "# 参照: github.com:rwightman/pytorch-image-models.\n",
        "class StochasticDepth(layers.Layer):\n",
        "    def __init__(self, drop_prop, **kwargs):\n",
        "        super(StochasticDepth, self).__init__(**kwargs)\n",
        "        self.drop_prob = drop_prop\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        if training:\n",
        "            keep_prob = 1 - self.drop_prob\n",
        "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
        "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
        "            random_tensor = tf.floor(random_tensor)\n",
        "            return (x / keep_prob) * random_tensor\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glIMo1O4-zp4"
      },
      "source": [
        "## トランスフォーマー用エンコーダのMLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-uppGB2C-zp5"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xy06jgx-zp5"
      },
      "source": [
        "## データの増強\n",
        "\n",
        "[原著論文](https://arxiv.org/abs/2104.05704)では、著者はより強い正則化を誘導するために\n",
        "[AutoAugment](https://arxiv.org/abs/1805.09501)を用いています。\n",
        "この例では、ランダムクロッピングやフリップなどの標準的な幾何学的拡張を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4f-aI7lh-zp5"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'tensorflow.keras.layers' has no attribute 'Rescaling'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/83/7ztpck1529700b59mz_yqpw00000gn/T/ipykernel_19734/750833007.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m data_augmentation = keras.Sequential(\n\u001b[1;32m      3\u001b[0m     [\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRescaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"horizontal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.layers' has no attribute 'Rescaling'"
          ]
        }
      ],
      "source": [
        "# リスケーリングレイヤーに注目。これらのレイヤーには、あらかじめ推論の動作が定義されています。\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Rescaling(scale=1.0 / 255),\n",
        "        layers.RandomCrop(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuEZQ64e-zp5"
      },
      "source": [
        "## 最終的なCCTモデル\n",
        "\n",
        "CCTで導入されたもう一つのレシピは、アテンションプーリングまたはシーケンスプーリングです。ViTでは\n",
        "クラストークンに対応する特徴量マップのみがプールされ、後続の分類タスク（または他の下流タスク）に使用されます。\n",
        "CCTでは、Transformerエンコーダからの出力に重み付けを行い、最終的なタスク固有のレイヤーに渡します（この例では分類を行います）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc9JhuqZ-zp5"
      },
      "outputs": [],
      "source": [
        "def create_cct_model(\n",
        "    image_size=image_size,\n",
        "    input_shape=input_shape,\n",
        "    num_heads=num_heads,\n",
        "    projection_dim=projection_dim,\n",
        "    transformer_units=transformer_units,\n",
        "):\n",
        "\n",
        "    inputs = layers.Input(input_shape)\n",
        "\n",
        "    # データの補強\n",
        "    augmented = data_augmentation(inputs)\n",
        "\n",
        "    # パッチのエンコード\n",
        "    cct_tokenizer = CCTTokenizer()\n",
        "    encoded_patches = cct_tokenizer(augmented)\n",
        "\n",
        "    # ポジショナルエンベッディングを適用\n",
        "    if positional_emb:\n",
        "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
        "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
        "        position_embeddings = pos_embed(positions)\n",
        "        encoded_patches += position_embeddings\n",
        "\n",
        "    # Stochastic Depthの確率を計算\n",
        "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
        "\n",
        "    # Transformerブロックのレイヤーを複数作成\n",
        "    for i in range(transformer_layers):\n",
        "        # レイヤーの正規化 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "\n",
        "        # マルチヘッドのアテンションレイヤーを作成\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "\n",
        "        # 接続をスキップする 1.\n",
        "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # レイヤーの正規化 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
        "\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "\n",
        "        # 接続をスキップする 2.\n",
        "        x3 = StochasticDepth(dpr[i])(x3)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # シーケンスプーリングの適用\n",
        "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
        "    weighted_representation = tf.matmul(\n",
        "        attention_weights, representation, transpose_a=True\n",
        "    )\n",
        "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
        "\n",
        "    # 出力を分類\n",
        "    logits = layers.Dense(num_classes)(weighted_representation)\n",
        "    # Kerasモデルを作成\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVulV3FF-zp6"
      },
      "source": [
        "## モデルの学習と評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu2vh_2Y-zp6"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.CategoricalCrossentropy(\n",
        "            from_logits=True, label_smoothing=0.1\n",
        "        ),\n",
        "        metrics=[\n",
        "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "cct_model = create_cct_model()\n",
        "history = run_experiment(cct_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JyrjdyO-zp6"
      },
      "source": [
        "それでは、モデルの学習状況を可視化してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjAlXhJz-zp6"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE5bWoJJ-zp6"
      },
      "source": [
        "今回学習したCCTモデルのパラメータ数はわずか**0.4万**で、30回のエポックタイムで**78%のトップ1精度を達成しました。\n",
        "上のプロットでは、オーバーフィッティングの兆候は見られません。\n",
        "これは、このネットワークを（おそらくもう少し正則化して）より長く訓練できることを意味しています。\n",
        "この性能は、コサイン減衰学習率スケジュールや、[AutoAugment](https://arxiv.org/abs/1805.09501)や[MixUp](https://arxiv.org/abs/1710.09412)、[Cutmix](https://arxiv.org/abs/1905.04899)のようなデータ増強技術によって、さらに性能を向上させることができます。\n",
        "これらの修正により、著者らは以下を発表しました。\n",
        "CIFAR-10データセットにおいて95.1%のトップ1精度を達成しました。また、著者らはいくつかの\n",
        "畳み込みブロックの数やトランスフォーマーの層などが、CCTの最終的な性能にどのように影響するかを研究するための実験も行っている。\n",
        "\n",
        "比較のために，ViTモデルは，CIFAR-10データセットでトップ-1精度に到達するために，約**470万**個のパラメータと**100エポック**の学習を経て、CIFAR-10データセットで78.22%のトップ1精度を達成しています。\n",
        "このモデルは[このノートブック](https://colab.research.google.com/gist/sayakpaul/1a80d9f582b044354a1a26c5cb3d69e5/image_classification_with_vision_transformer.ipynb)\n",
        "を参照してください。\n",
        "\n",
        "また、コンパクトな畳み込みトランスフォーマーの性能をNLPタスクで実証しています。\n",
        "NLPタスクでの性能を実証し、競争力のある結果を報告しています。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cct",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8ce8a6816ef3d6b182573c341c55e14ffc7e4f1d4a0b5fcb4bb3525f1150a4f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('tf': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
