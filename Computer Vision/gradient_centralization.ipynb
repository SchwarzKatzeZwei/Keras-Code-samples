{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCUijpkIrROP"
      },
      "source": [
        "# Gradient Centralization for Better Training Performance (勾配集中化による学習性能の向上)\n",
        "\n",
        "**Author:** [Rishit Dagli](https://github.com/Rishit-dagli)<br>\n",
        "**Date created:** 06/18/21<br>\n",
        "**Last modified:** 06/18/21<br>\n",
        "**Description:** Implement Gradient Centralization to improve training performance of DNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c841d7yfrROR"
      },
      "source": [
        "## 序章\n",
        "\n",
        "この例では、YongらによるDeep Neural Networksの新しい最適化手法である[Gradient Centralization](https://arxiv.org/abs/2004.01461)を実装し、\n",
        "Laurence Moroneyの[Horses or Humansans]で実証しています。\n",
        "Laurence Moroney氏の[Horses or Humans Dataset](https://www.tensorflow.org/datasets/catalog/horses_or_humans)で実証します。\n",
        "勾配集中化することで、学習プロセスの高速化とDNNの最終的な汎化性能の向上の両方を実現できます。\n",
        "これは、勾配のベクトルがゼロ平均になるように集中化することで、勾配を直接操作するものです。\n",
        "勾配集中化はさらに、損失関数とその勾配のリプシッツ性を改善することで、学習プロセスをより効率的で安定したものとなります。\n",
        "\n",
        "この例では、TensorFlow 2.2以降と、次のコマンドでインストールできる`tensorflow_datasets`が必要です。\n",
        "下記コマンドでインストールすることができます。\n",
        "\n",
        "```sh\n",
        "pip install tensorflow-datasets\n",
        "```\n",
        "\n",
        "この例では、グラデーション集中化を実装していますが、私が作ったパッケージでも\n",
        "私が作ったパッケージを使えば、簡単に使うことができます。\n",
        "[gradient-centralization-tf](https://github.com/Rishit-dagli/Gradient-Centralization-TensorFlow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIy9h6svrROR"
      },
      "source": [
        "## 設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0maX0G5YrROS"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmPtLZDSrROS"
      },
      "source": [
        "## データの準備\n",
        "\n",
        "今回の例では、[Horses or Humans dataset](https://www.tensorflow.org/datasets/catalog/horses_or_humans)を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPz1qM10rROT"
      },
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "input_shape = (300, 300, 3)\n",
        "dataset_name = \"horses_or_humans\"\n",
        "batch_size = 128\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "(train_ds, test_ds), metadata = tfds.load(\n",
        "    name=dataset_name,\n",
        "    split=[tfds.Split.TRAIN, tfds.Split.TEST],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "print(f\"Image shape: {metadata.features['image'].shape}\")\n",
        "print(f\"Training images: {metadata.splits['train'].num_examples}\")\n",
        "print(f\"Test images: {metadata.splits['test'].num_examples}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oOs6wITrROT"
      },
      "source": [
        "## データオーグメンテーションの活用\n",
        "\n",
        "ここでは、データを `[0, 1]` にリスケールし、データに簡単な補強を施します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCS-p--BrROU"
      },
      "outputs": [],
      "source": [
        "rescale = layers.Rescaling(1.0 / 255)\n",
        "\n",
        "data_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "        layers.RandomRotation(0.3),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def prepare(ds, shuffle=False, augment=False):\n",
        "    # Rescale dataset\n",
        "    ds = ds.map(lambda x, y: (rescale(x), y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(1024)\n",
        "\n",
        "    # Batch dataset\n",
        "    ds = ds.batch(batch_size)\n",
        "\n",
        "    # Use data augmentation only on the training set\n",
        "    if augment:\n",
        "        ds = ds.map(\n",
        "            lambda x, y: (data_augmentation(x, training=True), y),\n",
        "            num_parallel_calls=AUTOTUNE,\n",
        "        )\n",
        "\n",
        "    # Use buffered prefecting\n",
        "    return ds.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D3cM47YrROU"
      },
      "source": [
        "データのリスケールとオーグメンテーション"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9oBRApSrROV"
      },
      "outputs": [],
      "source": [
        "train_ds = prepare(train_ds, shuffle=True, augment=True)\n",
        "test_ds = prepare(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEfl0QFrrROV"
      },
      "source": [
        "## モデル定義\n",
        "\n",
        "ここでは、Convolutional neural networkの定義を説明します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztcwledlrROV"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(300, 300, 3)),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(512, activation=\"relu\"),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tkRYBYLrROV"
      },
      "source": [
        "## 勾配集中法の実装\n",
        "\n",
        "今後はオプティマイザクラスである`RMSProp`をサブクラス化します。\n",
        "クラスをサブクラス化し、`tf.keras.optimizers.Optimizer.get_gradients()`メソッドを変更して、勾配集中化しています。\n",
        "簡単に説明すると、例えば、Double-Double-Doubleの逆伝播で勾配を得るとします。\n",
        "密層や畳み込み層のバックプロポーゲーションによって勾配を得たとすると、次に重み行列の列ベクトルの平均を計算します。\n",
        "重み行列の列ベクトルの平均を計算し、各列ベクトルから平均を取り除きます。\n",
        "\n",
        "[本論文](https://arxiv.org/abs/2004.01461)では、様々なアプリケーションに関する実験を行っています。\n",
        "一般的な画像分類、細分化された画像分類、検出とセグメンテーション、同一人物見地の実験では、GCがDNN学習の性能を一貫して向上させることができることを示しています。\n",
        "\n",
        "また、現時点ではシンプルにするために、グラデーションのクリッピング機能は実装していません。\n",
        "しかし、これは非常に簡単に実装できます。\n",
        "\n",
        "現時点では、`RMSProp`オプティマイザーのサブクラスを作成しています。\n",
        "しかし、他のオプティマイザーやカスタムオプティマイザーでも同じように簡単に再現できます。\n",
        "オプティマイザーでも同じように再現できます。このクラスは、後のセクションで、次のように使用します。\n",
        "勾配集中法でモデルを学習する際に使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt-Uw5J3rROW"
      },
      "outputs": [],
      "source": [
        "class GCRMSprop(RMSprop):\n",
        "    def get_gradients(self, loss, params):\n",
        "        # We here just provide a modified get_gradients() function since we are\n",
        "        # trying to just compute the centralized gradients.\n",
        "\n",
        "        grads = []\n",
        "        gradients = super().get_gradients()\n",
        "        for grad in gradients:\n",
        "            grad_len = len(grad.shape)\n",
        "            if grad_len > 1:\n",
        "                axis = list(range(grad_len - 1))\n",
        "                grad -= tf.reduce_mean(grad, axis=axis, keep_dims=True)\n",
        "            grads.append(grad)\n",
        "\n",
        "        return grads\n",
        "\n",
        "\n",
        "optimizer = GCRMSprop(learning_rate=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOsA-6porROW"
      },
      "source": [
        "## トレーニングユーティリティー\n",
        "\n",
        "また、コールバックを作成して、トレーニングの合計時間と各エポックにかかった時間を簡単に測定できるようにします。\n",
        "効果を比較することに興味があるからです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yMj-PUMrROW"
      },
      "outputs": [],
      "source": [
        "class TimeHistory(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, batch, logs={}):\n",
        "        self.epoch_time_start = time()\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.times.append(time() - self.epoch_time_start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYFF0U64rROW"
      },
      "source": [
        "## GCを使用しないモデルのトレーニング\n",
        "\n",
        "次に、先ほど構築したモデルをGradient Centralizationなしでトレーニングします。\n",
        "これを、Gradient Centralizationを用いて学習したモデルの学習性能と比較します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA21LlzArROW"
      },
      "outputs": [],
      "source": [
        "time_callback_no_gc = TimeHistory()\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=RMSprop(learning_rate=1e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98145xmJrROX"
      },
      "source": [
        "また、履歴を保存しておくことで、勾配集中法で学習したモデルとそうでないモデルを後で比較することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aCcj6UMrROX"
      },
      "outputs": [],
      "source": [
        "history_no_gc = model.fit(\n",
        "    train_ds, epochs=10, verbose=1, callbacks=[time_callback_no_gc]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVdr9FMlrROX"
      },
      "source": [
        "## GCでモデルを学習\n",
        "\n",
        "同じモデルを、今度はGradient Centralizationを使ってトレーニングします。\n",
        "今回は、オプティマイザーがGradient Centralizationを使用していることに注目してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf-ght8LrROX"
      },
      "outputs": [],
      "source": [
        "time_callback_gc = TimeHistory()\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history_gc = model.fit(train_ds, epochs=10, verbose=1, callbacks=[time_callback_gc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFKwlYCyrROX"
      },
      "source": [
        "## パフォーマンスの比較"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DNtZJVsrROX"
      },
      "outputs": [],
      "source": [
        "print(\"Not using Gradient Centralization\")\n",
        "print(f\"Loss: {history_no_gc.history['loss'][-1]}\")\n",
        "print(f\"Accuracy: {history_no_gc.history['accuracy'][-1]}\")\n",
        "print(f\"Training Time: {sum(time_callback_no_gc.times)}\")\n",
        "\n",
        "print(\"Using Gradient Centralization\")\n",
        "print(f\"Loss: {history_gc.history['loss'][-1]}\")\n",
        "print(f\"Accuracy: {history_gc.history['accuracy'][-1]}\")\n",
        "print(f\"Training Time: {sum(time_callback_gc.times)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0CvFAoUrROX"
      },
      "source": [
        "読者の皆様には、様々な分野のデータセットでGradient Centralizationを試し、その効果を試すことをお勧めします。\n",
        "その効果を試してみてください。また、[原著論文](https://arxiv.org/abs/2004.01461)をご覧になることを強くお勧めします。\n",
        "Gradient Centralizationに関するいくつかの研究を紹介しています。\n",
        "性能、汎用性、学習時間をいかに改善できるかを示していますし、より効率的です。\n",
        "\n",
        "この実装をレビューしてくれた[Ali Mustufa Shaikh](https://github.com/ialimustufa)に感謝します。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "gradient_centralization",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
